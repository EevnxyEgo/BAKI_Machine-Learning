{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Drawing helpers\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup important landmarks and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine important landmarks for lunge\n",
    "IMPORTANT_LMS = [\n",
    "    \"NOSE\",\n",
    "    \"LEFT_SHOULDER\",\n",
    "    \"RIGHT_SHOULDER\",\n",
    "    \"LEFT_HIP\",\n",
    "    \"RIGHT_HIP\",\n",
    "    \"LEFT_KNEE\",\n",
    "    \"RIGHT_KNEE\",\n",
    "    \"LEFT_ANKLE\",\n",
    "    \"RIGHT_ANKLE\",\n",
    "    \"LEFT_HEEL\",\n",
    "    \"RIGHT_HEEL\",\n",
    "    \"LEFT_FOOT_INDEX\",\n",
    "    \"RIGHT_FOOT_INDEX\",\n",
    "]\n",
    "\n",
    "# Generate all columns of the data frame\n",
    "\n",
    "HEADERS = [\"label\"] # Label column\n",
    "\n",
    "for lm in IMPORTANT_LMS:\n",
    "    HEADERS += [f\"{lm.lower()}_x\", f\"{lm.lower()}_y\", f\"{lm.lower()}_z\", f\"{lm.lower()}_v\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_important_keypoints(results) -> list:\n",
    "    '''\n",
    "    Extract important keypoints from mediapipe pose detection\n",
    "    '''\n",
    "    landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "    data = []\n",
    "    for lm in IMPORTANT_LMS:\n",
    "        keypoint = landmarks[mp_pose.PoseLandmark[lm].value]\n",
    "        data.append([keypoint.x, keypoint.y, keypoint.z, keypoint.visibility])\n",
    "    \n",
    "    return np.array(data).flatten().tolist()\n",
    "\n",
    "\n",
    "def rescale_frame(frame, percent=50):\n",
    "    '''\n",
    "    Rescale a frame to a certain percentage compare to its original frame\n",
    "    '''\n",
    "    width = int(frame.shape[1] * percent/ 100)\n",
    "    height = int(frame.shape[0] * percent/ 100)\n",
    "    dim = (width, height)\n",
    "    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "def calculate_angle(point1: list, point2: list, point3: list) -> float:\n",
    "    '''\n",
    "    Calculate the angle between 3 points\n",
    "    Unit of the angle will be in Degree\n",
    "    '''\n",
    "    point1 = np.array(point1)\n",
    "    point2 = np.array(point2)\n",
    "    point3 = np.array(point3)\n",
    "\n",
    "    # Calculate algo\n",
    "    angleInRad = np.arctan2(point3[1] - point2[1], point3[0] - point2[0]) - np.arctan2(point1[1] - point2[1], point1[0] - point2[0])\n",
    "    angleInDeg = np.abs(angleInRad * 180.0 / np.pi)\n",
    "\n",
    "    angleInDeg = angleInDeg if angleInDeg <= 180 else 360 - angleInDeg\n",
    "    return angleInDeg\n",
    "    \n",
    "\n",
    "def analyze_knee_angle(\n",
    "    mp_results, stage: str, angle_thresholds: list, draw_to_image: tuple = None\n",
    "):\n",
    "\n",
    "    results = {\n",
    "        \"error\": None,\n",
    "        \"right\": {\"error\": None, \"angle\": None},\n",
    "        \"left\": {\"error\": None, \"angle\": None},\n",
    "    }\n",
    "\n",
    "    landmarks = mp_results.pose_landmarks.landmark\n",
    "\n",
    "    # Calculate right knee angle\n",
    "    right_hip = [\n",
    "        landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "        landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y,\n",
    "    ]\n",
    "    right_knee = [\n",
    "        landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\n",
    "        landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y,\n",
    "    ]\n",
    "    right_ankle = [\n",
    "        landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "        landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y,\n",
    "    ]\n",
    "    results[\"right\"][\"angle\"] = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "\n",
    "    # Calculate left knee angle\n",
    "    left_hip = [\n",
    "        landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "        landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y,\n",
    "    ]\n",
    "    left_knee = [\n",
    "        landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x,\n",
    "        landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y,\n",
    "    ]\n",
    "    left_ankle = [\n",
    "        landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x,\n",
    "        landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y,\n",
    "    ]\n",
    "    results[\"left\"][\"angle\"] = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "\n",
    "    # Draw to image\n",
    "    if draw_to_image is not None and stage != \"down\":\n",
    "        (image, video_dimensions) = draw_to_image\n",
    "\n",
    "        # Visualize angles\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            str(int(results[\"right\"][\"angle\"])),\n",
    "            tuple(np.multiply(right_knee, video_dimensions).astype(int)),\n",
    "            cv2.FONT_HERSHEY_COMPLEX,\n",
    "            0.5,\n",
    "            (255, 255, 255),\n",
    "            1,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            str(int(results[\"left\"][\"angle\"])),\n",
    "            tuple(np.multiply(left_knee, video_dimensions).astype(int)),\n",
    "            cv2.FONT_HERSHEY_COMPLEX,\n",
    "            0.5,\n",
    "            (255, 255, 255),\n",
    "            1,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    if stage != \"down\":\n",
    "        return results\n",
    "\n",
    "    # Evaluation\n",
    "    results[\"error\"] = False\n",
    "\n",
    "    if angle_thresholds[0] <= results[\"right\"][\"angle\"] <= angle_thresholds[1]:\n",
    "        results[\"right\"][\"error\"] = False\n",
    "    else:\n",
    "        results[\"right\"][\"error\"] = True\n",
    "        results[\"error\"] = True\n",
    "\n",
    "    if angle_thresholds[0] <= results[\"left\"][\"angle\"] <= angle_thresholds[1]:\n",
    "        results[\"left\"][\"error\"] = False\n",
    "    else:\n",
    "        results[\"left\"][\"error\"] = True\n",
    "        results[\"error\"] = True\n",
    "\n",
    "    # Draw to image\n",
    "    if draw_to_image is not None:\n",
    "        (image, video_dimensions) = draw_to_image\n",
    "\n",
    "        right_color = (255, 255, 255) if not results[\"right\"][\"error\"] else (0, 0, 255)\n",
    "        left_color = (255, 255, 255) if not results[\"left\"][\"error\"] else (0, 0, 255)\n",
    "\n",
    "        right_font_scale = 0.5 if not results[\"right\"][\"error\"] else 1\n",
    "        left_font_scale = 0.5 if not results[\"left\"][\"error\"] else 1\n",
    "\n",
    "        right_thickness = 1 if not results[\"right\"][\"error\"] else 2\n",
    "        left_thickness = 1 if not results[\"left\"][\"error\"] else 2\n",
    "\n",
    "        # Visualize angles\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            str(int(results[\"right\"][\"angle\"])),\n",
    "            tuple(np.multiply(right_knee, video_dimensions).astype(int)),\n",
    "            cv2.FONT_HERSHEY_COMPLEX,\n",
    "            right_font_scale,\n",
    "            right_color,\n",
    "            right_thickness,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            str(int(results[\"left\"][\"angle\"])),\n",
    "            tuple(np.multiply(left_knee, video_dimensions).astype(int)),\n",
    "            cv2.FONT_HERSHEY_COMPLEX,\n",
    "            left_font_scale,\n",
    "            left_color,\n",
    "            left_thickness,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO_PATH1 = \"../data/lunge/lunge_test_3.mp4\"\n",
    "# VIDEO_PATH2 = \"../data/lunge/lunge_test_5.mp4\"\n",
    "VIDEO_PATH = \"../../demo/lunges.mp4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./model/input_scaler.pkl\", \"rb\") as f:\n",
    "    input_scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detection with Sklearn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(\"./model/sklearn/stage_SVC_model.pkl\", \"rb\") as f:\n",
    "    stage_sklearn_model = pickle.load(f)\n",
    "\n",
    "with open(\"./model/sklearn/err_LR_model.pkl\", \"rb\") as f:\n",
    "    err_sklearn_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "current_stage = \"\"\n",
    "counter = 0\n",
    "\n",
    "prediction_probability_threshold = 0.8\n",
    "ANGLE_THRESHOLDS = [60, 135]\n",
    "\n",
    "knee_over_toe = False\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def draw_transparent_rect(img, top_left, bottom_right, color, alpha=0.6):\n",
    "    overlay = img.copy()\n",
    "    cv2.rectangle(overlay, top_left, bottom_right, color, -1)\n",
    "    return cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0)\n",
    "\n",
    "def display_text(image, text, pos, font_scale=0.6, color=(255, 255, 255), thickness=1):\n",
    "    cv2.putText(image, text, pos, cv2.FONT_HERSHEY_COMPLEX, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Reduce size of a frame\n",
    "        image = rescale_frame(image, 50)\n",
    "        video_dimensions = [image.shape[1], image.shape[0]]\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        results = pose.process(image)\n",
    "\n",
    "        if not results.pose_landmarks:\n",
    "            print(\"No human found\")\n",
    "            continue\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw landmarks and connections with modern styling\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            results.pose_landmarks, \n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(0, 150, 255), thickness=3, circle_radius=4), \n",
    "            mp_drawing.DrawingSpec(color=(50, 50, 255), thickness=2, circle_radius=2)\n",
    "        )\n",
    "\n",
    "        # Make detection\n",
    "        try:\n",
    "            # Extract keypoints from frame for the input\n",
    "            row = extract_important_keypoints(results)\n",
    "            X = pd.DataFrame([row], columns=HEADERS[1:])\n",
    "            X = pd.DataFrame(input_scaler.transform(X))\n",
    "\n",
    "            # Make prediction and its probability\n",
    "            stage_predicted_class = stage_sklearn_model.predict(X)[0]\n",
    "            stage_prediction_probabilities = stage_sklearn_model.predict_proba(X)[0]\n",
    "            stage_prediction_probability = round(stage_prediction_probabilities[stage_prediction_probabilities.argmax()], 2)\n",
    "\n",
    "            # Evaluate model prediction\n",
    "            if stage_predicted_class == \"I\" and stage_prediction_probability >= prediction_probability_threshold:\n",
    "                current_stage = \"init\"\n",
    "            elif stage_predicted_class == \"M\" and stage_prediction_probability >= prediction_probability_threshold: \n",
    "                current_stage = \"mid\"\n",
    "            elif stage_predicted_class == \"D\" and stage_prediction_probability >= prediction_probability_threshold:\n",
    "                if current_stage in [\"mid\", \"init\"]:\n",
    "                    counter += 1\n",
    "                current_stage = \"down\"\n",
    "            \n",
    "            # Error detection\n",
    "            # Knee angle\n",
    "            analyze_knee_angle(mp_results=results, stage=current_stage, angle_thresholds=ANGLE_THRESHOLDS, draw_to_image=(image, video_dimensions))\n",
    "\n",
    "            # Knee over toe\n",
    "            err_predicted_class = err_prediction_probabilities = err_prediction_probability = None\n",
    "            if current_stage == \"down\":\n",
    "                err_predicted_class = err_sklearn_model.predict(X)[0]\n",
    "                err_prediction_probabilities = err_sklearn_model.predict_proba(X)[0]\n",
    "                err_prediction_probability = round(err_prediction_probabilities[err_prediction_probabilities.argmax()], 2)\n",
    "                \n",
    "            # Visualization with a modern look\n",
    "            # Draw semi-transparent background\n",
    "            image = draw_transparent_rect(image, (0, 0), (800, 50), (45, 45, 45), alpha=0.75)\n",
    "\n",
    "            # Display stage prediction\n",
    "            display_text(image, \"STAGE\", (15, 20), font_scale=0.7, color=(0, 200, 255), thickness=2)\n",
    "            display_text(image, f\"{stage_prediction_probability:.2f} - {current_stage}\", (15, 40), font_scale=0.7, color=(255, 255, 255), thickness=2)\n",
    "\n",
    "            # Display error prediction (Knee Over Toe - K_O_T)\n",
    "            display_text(image, \"K_O_T\", (200, 20), font_scale=0.7, color=(0, 200, 255), thickness=2)\n",
    "            if err_predicted_class is not None:\n",
    "                display_text(image, f\"{err_prediction_probability:.2f} - {err_predicted_class}\", (200, 40), font_scale=0.7, color=(255, 255, 255), thickness=2)\n",
    "            else:\n",
    "                display_text(image, \"N/A\", (200, 40), font_scale=0.7, color=(255, 255, 255), thickness=2)\n",
    "\n",
    "            # Display Counter\n",
    "            display_text(image, \"COUNTER\", (350, 20), font_scale=0.7, color=(0, 200, 255), thickness=2)\n",
    "            display_text(image, str(counter), (350, 40), font_scale=0.7, color=(255, 255, 255), thickness=2)\n",
    "\n",
    "            # Optional improvement for message clarity (e.g., human presence message)\n",
    "            if not results.pose_landmarks:\n",
    "                display_text(image, \"No Human Found\", (500, 20), font_scale=0.7, color=(255, 50, 50), thickness=2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            break\n",
    "        \n",
    "        cv2.imshow(\"CV2\", image)\n",
    "        \n",
    "        # Press Q to close cv2 window\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # (Optional)Fix bugs cannot close windows in MacOS (https://stackoverflow.com/questions/6116564/destroywindow-does-not-close-window-on-mac-using-python-and-opencv)\n",
    "    for i in range (1, 5):\n",
    "        cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Visualization\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Draw rounded rectangles for status box with opacity\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m overlay \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m()\n\u001b[0;32m      4\u001b[0m cv2\u001b[38;5;241m.\u001b[39mrectangle(overlay, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m50\u001b[39m), (\u001b[38;5;241m245\u001b[39m, \u001b[38;5;241m117\u001b[39m, \u001b[38;5;241m16\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "# Visualization\n",
    "# Draw rounded rectangles for status box with opacity\n",
    "overlay = image.copy()\n",
    "cv2.rectangle(overlay, (0, 0), (800, 50), (245, 117, 16), -1)\n",
    "alpha = 0.7\n",
    "cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0, image)\n",
    "\n",
    "# Display Stage Prediction\n",
    "cv2.putText(image, \"STAGE\", (15, 15), cv2.FONT_HERSHEY_COMPLEX, 0.6, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "cv2.putText(image, f\"{stage_prediction_probability:.2f} - {current_stage}\", (15, 35), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "# Display Error Prediction (Knee Over Toe - K_O_T)\n",
    "cv2.putText(image, \"K_O_T\", (200, 15), cv2.FONT_HERSHEY_COMPLEX, 0.6, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "if err_predicted_class is not None:\n",
    "    cv2.putText(image, f\"{err_prediction_probability:.2f} - {err_predicted_class}\", (200, 35), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "else:\n",
    "    cv2.putText(image, \"N/A\", (200, 35), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "# Display Counter\n",
    "cv2.putText(image, \"COUNTER\", (350, 15), cv2.FONT_HERSHEY_COMPLEX, 0.6, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "cv2.putText(image, str(counter), (350, 35), cv2.FONT_HERSHEY_COMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "# Optional improvement for message clarity (e.g., human presence message)\n",
    "if not results.pose_landmarks:\n",
    "    cv2.putText(image, \"No Human Found\", (450, 15), cv2.FONT_HERSHEY_COMPLEX, 0.6, (0, 0, 255), 2, cv2.LINE_AA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detection with deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 08:33:52.012812: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-07-13 08:33:52.012924: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "with open(\"./model/dp/err_lunge_dp.pkl\", \"rb\") as f:\n",
    "    err_deep_learning_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 08:34:20.840652: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-07-13 08:34:20.877485: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "current_stage = \"\"\n",
    "counter = 0\n",
    "\n",
    "prediction_probability_threshold = 0.8\n",
    "ANGLE_THRESHOLDS = [60, 135]\n",
    "\n",
    "knee_over_toe = False\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Reduce size of a frame\n",
    "        image = rescale_frame(image, 50)\n",
    "        video_dimensions = [image.shape[1], image.shape[0]]\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        results = pose.process(image)\n",
    "\n",
    "        if not results.pose_landmarks:\n",
    "            print(\"No human found\")\n",
    "            continue\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw landmarks and connections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(244, 117, 66), thickness=2, circle_radius=2), mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=1))\n",
    "\n",
    "        # Make detection\n",
    "        try:\n",
    "            # Extract keypoints from frame for the input\n",
    "            row = extract_important_keypoints(results)\n",
    "            X = pd.DataFrame([row], columns=HEADERS[1:])\n",
    "            X = pd.DataFrame(input_scaler.transform(X))\n",
    "\n",
    "            # Make prediction and its probability\n",
    "            stage_predicted_class = stage_sklearn_model.predict(X)[0]\n",
    "            stage_prediction_probabilities = stage_sklearn_model.predict_proba(X)[0]\n",
    "            stage_prediction_probability = round(stage_prediction_probabilities[stage_prediction_probabilities.argmax()], 2)\n",
    "\n",
    "            # Evaluate model prediction\n",
    "            if stage_predicted_class == \"I\" and stage_prediction_probability >= prediction_probability_threshold:\n",
    "                current_stage = \"init\"\n",
    "            elif stage_predicted_class == \"M\" and stage_prediction_probability >= prediction_probability_threshold: \n",
    "                current_stage = \"mid\"\n",
    "            elif stage_predicted_class == \"D\" and stage_prediction_probability >= prediction_probability_threshold:\n",
    "                if current_stage == \"mid\":\n",
    "                    counter += 1\n",
    "                \n",
    "                current_stage = \"down\"\n",
    "            \n",
    "            # Error detection\n",
    "            # Knee angle\n",
    "            analyze_knee_angle(mp_results=results, stage=current_stage, angle_thresholds=ANGLE_THRESHOLDS, draw_to_image=(image, video_dimensions))\n",
    "\n",
    "            # Knee over toe\n",
    "            err_predicted_class = err_prediction_probabilities = err_prediction_probability = None\n",
    "            if current_stage == \"down\":\n",
    "                err_prediction = err_deep_learning_model.predict(X, verbose=False)\n",
    "                err_predicted_class = np.argmax(err_prediction, axis=1)[0]\n",
    "                err_prediction_probability = round(max(err_prediction.tolist()[0]), 2)\n",
    "\n",
    "                err_predicted_class = \"C\" if err_predicted_class == 1 else \"L\"\n",
    "                \n",
    "            \n",
    "            # Visualization\n",
    "            # Status box\n",
    "            cv2.rectangle(image, (0, 0), (800, 45), (245, 117, 16), -1)\n",
    "\n",
    "            # Display stage prediction\n",
    "            cv2.putText(image, \"STAGE\", (15, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(stage_prediction_probability), (10, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, current_stage, (50, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            # Display error prediction\n",
    "            cv2.putText(image, \"K_O_T\", (200, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(err_prediction_probability), (195, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(err_predicted_class), (245, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            # Display Counter\n",
    "            cv2.putText(image, \"COUNTER\", (110, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(counter), (110, 30), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            break\n",
    "        \n",
    "        cv2.imshow(\"CV2\", image)\n",
    "        \n",
    "        # Press Q to close cv2 window\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # (Optional)Fix bugs cannot close windows in MacOS (https://stackoverflow.com/questions/6116564/destroywindow-does-not-close-window-on-mac-using-python-and-opencv)\n",
    "    for i in range (1, 5):\n",
    "        cv2.waitKey(1)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "- For stage detection:\n",
    "    - Best Sklearn model: KNN\n",
    "- For error detection:\n",
    "    - Best Sklearn model: LR\n",
    "    - Both models are correct most of the time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
